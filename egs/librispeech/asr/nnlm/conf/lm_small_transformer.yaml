
gpu: 1
tensorboard_dir: 'exp-nnlm/tensorobard'

# network architecture equivalent configuration to
# https://github.com/pytorch/examples/blob/master/word_language_model/main.py
model_module: transformer
transformer_conf:
  embed_unit: 200
  attention_heads: 8
  nlayers: 16
  linear_units: 2048
  dropout: 0.2

shared_conf:
  ntoken: 5003

# Now using Noam optimizer and tuning configuration
# optimizer_conf:
#   # for Adam
#   lr: 0.0003
#   weight_decay: 0.001
#   # for SGD
#   # lr: 0.01
#   # weight_decay: 0.001

trainer_conf:
  num_epochs: 60
  clip: 0.25
  model_dir: './exp-nnlm/models/'


dataset_conf:
  train_token: 'data/nnlm/text/librispeech.txt.tokens'
  dev_token: 'data/nnlm/text/dev.txt.tokens'

dataloader_conf:
  train:
    batch_size: 256
    num_workers: 0
    drop_last: True
  dev:
    batch_size: 20
    num_workers: 0
    drop_last: False
